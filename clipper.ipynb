{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2095d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pytubefix import YouTube # Using pytubefix\n",
    "# from pytubefix.exceptions import PytubeFixError # Check actual exception name if needed for specific pytubefix errors\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "from pydantic import BaseModel, Field, RootModel # For Pydantic V2\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "import subprocess\n",
    "from typing import List, Optional\n",
    "from dotenv import load_dotenv\n",
    "import logging # For more structured logging\n",
    "\n",
    "# Configure basic logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11856426",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    logging.error(\"OpenAI API key not found. Please set it in your .env file.\")\n",
    "    # You might want to raise an exception here or handle it more gracefully\n",
    "    # raise ValueError(\"OpenAI API key not found.\")\n",
    "else:\n",
    "    logging.info(\"OpenAI API key loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26782a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- User-Defined Configuration ---\n",
    "youtube_url = \"https://youtu.be/nvuAt8sl7Ag?si=6x3KLf63BttTX_qx&utm_source=ZTQxO\"\n",
    "video_download_directory = \"downloaded_video\"\n",
    "clips_output_directory = \"generated_clips\"\n",
    "desired_video_resolution = \"720p\" # e.g., \"720p\", \"360p\", or None for best progressive\n",
    "segment_duration_min_seconds = 50\n",
    "segment_duration_max_seconds = 59\n",
    "max_topics_to_identify = 5 # How many topics to ask the first LLM for\n",
    "\n",
    "# --- Global State Variables (initialized) ---\n",
    "downloaded_video_path = None\n",
    "video_base_title = None\n",
    "raw_transcript_data = None      \n",
    "transcript_file_path = None\n",
    "identified_topics: List[str] = []\n",
    "all_extracted_segments: List[dict] = [] # Will store Segment-like dictionaries\n",
    "\n",
    "logging.info(f\"Configuration loaded. Target YouTube URL: {youtube_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d28d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ensure Directories Exist ---\n",
    "if not os.path.exists(video_download_directory):\n",
    "    os.makedirs(video_download_directory)\n",
    "    logging.info(f\"Created directory: {video_download_directory}\")\n",
    "\n",
    "if not os.path.exists(clips_output_directory):\n",
    "    os.makedirs(clips_output_directory)\n",
    "    logging.info(f\"Created directory: {clips_output_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe362d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download Video ---\n",
    "try:\n",
    "    logging.info(f\"Fetching video info for: {youtube_url}\")\n",
    "    yt = YouTube(youtube_url)\n",
    "\n",
    "    video_title_raw = yt.title\n",
    "    # Sanitize title for filename: Keep alphanumeric, spaces, hyphens, underscores. Replace others with underscore.\n",
    "    video_base_title_temp = \"\".join(c if c.isalnum() or c in (' ', '-', '_') else '_' for c in video_title_raw).rstrip()\n",
    "    video_base_title_temp = video_base_title_temp.replace(\" \", \"_\")\n",
    "    if not video_base_title_temp: # Handle cases where title becomes empty\n",
    "        video_base_title_temp = f\"youtube_video_{yt.video_id}\"\n",
    "    video_base_title = video_base_title_temp # Assign to global variable\n",
    "\n",
    "    video_filename_mp4 = f\"{video_base_title}.mp4\"\n",
    "    downloaded_video_path_temp = os.path.join(video_download_directory, video_filename_mp4)\n",
    "\n",
    "    logging.info(f\"Video Title: {video_title_raw}\")\n",
    "    logging.info(f\"Sanitized base title: {video_base_title}\")\n",
    "    logging.info(f\"Attempting to download video to: {downloaded_video_path_temp}\")\n",
    "\n",
    "    # Check if video already exists to avoid re-downloading\n",
    "    if os.path.exists(downloaded_video_path_temp):\n",
    "        logging.info(f\"Video already exists at {downloaded_video_path_temp}. Skipping download.\")\n",
    "        downloaded_video_path = downloaded_video_path_temp\n",
    "    else:\n",
    "        streams_query = yt.streams.filter(progressive=True, file_extension='mp4')\n",
    "        stream_to_download = None\n",
    "        if desired_video_resolution:\n",
    "            stream_to_download = streams_query.filter(res=desired_video_resolution).first()\n",
    "            if not stream_to_download:\n",
    "                logging.warning(f\"Resolution {desired_video_resolution} not found as progressive mp4. Trying best available.\")\n",
    "                stream_to_download = streams_query.order_by('resolution').desc().first()\n",
    "        else:\n",
    "            stream_to_download = streams_query.order_by('resolution').desc().first()\n",
    "\n",
    "        if stream_to_download:\n",
    "            logging.info(f\"Found video stream: Resolution {stream_to_download.resolution}, MIME Type {stream_to_download.mime_type}\")\n",
    "            stream_to_download.download(output_path=video_download_directory, filename=video_filename_mp4)\n",
    "            logging.info(f\"Successfully downloaded video: {downloaded_video_path_temp}\")\n",
    "            downloaded_video_path = downloaded_video_path_temp # Assign to global\n",
    "        else:\n",
    "            logging.error(\"No suitable progressive MP4 stream found for this video.\")\n",
    "            # Consider raising an error to stop execution if video is essential\n",
    "\n",
    "except Exception as e: # Catching a broader exception for pytubefix initially\n",
    "    logging.error(f\"An error occurred during video download: {e}\", exc_info=True)\n",
    "    # Consider how to handle this - maybe exit or skip subsequent steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248fe260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fetch and Save Transcript ---\n",
    "if downloaded_video_path and video_base_title: # Proceed only if video download was successful\n",
    "    try:\n",
    "        video_id = yt.video_id # yt object should still be in scope from Cell 5\n",
    "        logging.info(f\"Fetching transcript for video ID: {video_id}...\")\n",
    "\n",
    "        transcript_filename_json = f\"{video_base_title}_transcript.json\"\n",
    "        transcript_file_path_temp = os.path.join(video_download_directory, transcript_filename_json)\n",
    "\n",
    "        # Check if transcript file already exists\n",
    "        if os.path.exists(transcript_file_path_temp):\n",
    "            logging.info(f\"Transcript file already exists: {transcript_file_path_temp}. Loading from file.\")\n",
    "            with open(transcript_file_path_temp, \"r\", encoding=\"utf-8\") as f:\n",
    "                raw_transcript_data_temp = json.load(f)\n",
    "        else:\n",
    "            raw_transcript_data_temp = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "            with open(transcript_file_path_temp, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(raw_transcript_data_temp, f, ensure_ascii=False, indent=4)\n",
    "            logging.info(f\"Successfully fetched and saved transcript to: {transcript_file_path_temp}\")\n",
    "\n",
    "        raw_transcript_data = raw_transcript_data_temp # Assign to global\n",
    "        transcript_file_path = transcript_file_path_temp # Assign to global\n",
    "\n",
    "        # Optional: Basic validation of transcript data\n",
    "        if isinstance(raw_transcript_data, list) and len(raw_transcript_data) > 0 and isinstance(raw_transcript_data[0], dict):\n",
    "            logging.info(f\"Transcript loaded/fetched successfully. Number of segments: {len(raw_transcript_data)}\")\n",
    "        else:\n",
    "            logging.warning(\"Transcript data seems to be empty or not in the expected format.\")\n",
    "            raw_transcript_data = None # Reset if invalid\n",
    "\n",
    "    except TranscriptsDisabled:\n",
    "        logging.error(f\"Transcripts are disabled for video: {youtube_url}\")\n",
    "        raw_transcript_data = None\n",
    "    except NoTranscriptFound:\n",
    "        logging.error(f\"No transcript found for video: {youtube_url}. It might be missing or not in a supported language.\")\n",
    "        raw_transcript_data = None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while fetching or saving the transcript: {e}\", exc_info=True)\n",
    "        raw_transcript_data = None\n",
    "else:\n",
    "    logging.warning(\"Skipping transcript fetching because video download failed or video_base_title was not set.\")\n",
    "\n",
    "# For the LLM, we might need a single string of the transcript text for some prompts\n",
    "# For topic identification, sending the whole structured transcript might be too much for cheaper models.\n",
    "# Let's create a concatenated text version.\n",
    "full_transcript_text = \"\"\n",
    "if raw_transcript_data:\n",
    "    full_transcript_text = \" \".join([segment['text'] for segment in raw_transcript_data])\n",
    "    logging.info(f\"Full transcript text concatenated. Length: {len(full_transcript_text)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c4f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pydantic Model for Topic Analysis ---\n",
    "class VideoAnalysis(BaseModel):\n",
    "    summary: str = Field(..., description=\"A brief summary of the video content, in 2-3 sentences.\")\n",
    "    main_topics: List[str] = Field(\n",
    "        ...,\n",
    "        description=f\"A list of {max_topics_to_identify} distinct main topics or themes discussed in the video. Each topic should be a short phrase (2-5 words) suitable for guiding clip extraction.\",\n",
    "        min_length=1, # Pydantic v2: min_items, for v1 it was min_items in Field. For Pydantic v2 List, use model_validator or check during Field\n",
    "        max_length=max_topics_to_identify # Pydantic v2: max_items\n",
    "    )\n",
    "    target_audience: Optional[str] = Field(None, description=\"The likely target audience for this video (e.g., 'Tech Enthusiasts', 'Business Leaders', 'Students').\")\n",
    "\n",
    "logging.info(\"Pydantic model 'VideoAnalysis' defined for topic identification.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db76dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM for Topic Identification ---\n",
    "identified_topics = [] # Reset global variable\n",
    "\n",
    "if OPENAI_API_KEY and full_transcript_text: # Proceed only if API key and transcript text exist\n",
    "    try:\n",
    "        logging.info(\"Initializing LLM for topic identification...\")\n",
    "        # Using a potentially faster/cheaper model for this broader task\n",
    "        llm_topic_identifier = ChatOpenAI(\n",
    "            api_key=OPENAI_API_KEY,\n",
    "            model=\"gpt-4o-mini\", # or \"gpt-3.5-turbo\"\n",
    "            temperature=0.3\n",
    "        )\n",
    "\n",
    "        topic_prompt_text = (\n",
    "            f\"Please analyze the following video transcript. Provide a brief summary of the video's content, \"\n",
    "            f\"identify exactly {max_topics_to_identify} distinct main topics or themes that would be suitable for creating engaging short video clips, \"\n",
    "            f\"and suggest a likely target audience for this video.\\n\\n\"\n",
    "            f\"Each topic should be a concise phrase (2-5 words).\\n\\n\"\n",
    "            f\"Respond ONLY in the requested JSON structure.\\n\\n\"\n",
    "            f\"Transcript Text:\\n\\\"\\\"\\\"\\n{full_transcript_text[:15000]}\\n\\\"\\\"\\\"\" # Send a truncated version if too long for this model\n",
    "        ) # Limiting to first 15k chars for topic ID to manage token limits & cost\n",
    "\n",
    "        topic_messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert video content analyst. Your task is to analyze a video transcript and identify its core themes. Respond ONLY with the requested JSON structure.\"},\n",
    "            {\"role\": \"user\", \"content\": topic_prompt_text}\n",
    "        ]\n",
    "\n",
    "        logging.info(\"Invoking LLM for topic identification...\")\n",
    "        structured_llm_topic = llm_topic_identifier.with_structured_output(VideoAnalysis)\n",
    "        analysis_result = structured_llm_topic.invoke(topic_messages)\n",
    "\n",
    "        if analysis_result:\n",
    "            logging.info(f\"Video Summary: {analysis_result.summary}\")\n",
    "            logging.info(f\"Identified Topics: {analysis_result.main_topics}\")\n",
    "            if analysis_result.target_audience:\n",
    "                logging.info(f\"Target Audience: {analysis_result.target_audience}\")\n",
    "            identified_topics = analysis_result.main_topics # Assign to global\n",
    "        else:\n",
    "            logging.warning(\"LLM for topic identification did not return a valid result.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during topic identification with LLM: {e}\", exc_info=True)\n",
    "else:\n",
    "    if not OPENAI_API_KEY:\n",
    "        logging.warning(\"OpenAI API key not available. Skipping topic identification.\")\n",
    "    if not full_transcript_text:\n",
    "        logging.warning(\"Transcript text not available. Skipping topic identification.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec0fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pydantic Models for Segment Extraction ---\n",
    "class Segment(BaseModel):\n",
    "    start_time: int = Field(..., description=\"The start time of the segment in SECONDS from the beginning of the video.\")\n",
    "    end_time: int = Field(..., description=\"The end time of the segment in SECONDS from the beginning of the video.\")\n",
    "    title: str = Field(..., description=\"A concise, engaging title for this video segment (max 10 words).\")\n",
    "    description: str = Field(..., description=\"A brief 1-2 sentence description of what this segment is about.\")\n",
    "    # Duration can be calculated: end_time - start_time.\n",
    "    # LLM should focus on getting start_time and end_time to match the duration constraints.\n",
    "\n",
    "class SegmentResponse(BaseModel): # Renamed for clarity\n",
    "    segments: List[Segment] = Field(..., description=\"A list of identified video segments matching the criteria.\")\n",
    "\n",
    "logging.info(\"Pydantic models 'Segment' and 'SegmentResponse' defined for segment extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d2976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM for Segment Extraction (Looping Per Topic) ---\n",
    "all_extracted_segments = [] # Reset global variable\n",
    "\n",
    "if OPENAI_API_KEY and raw_transcript_data and identified_topics: # Proceed if all prereqs are met\n",
    "    try:\n",
    "        logging.info(\"Initializing LLM for segment extraction...\")\n",
    "        # Using a potentially more powerful model for precision in segmenting\n",
    "        llm_segment_extractor = ChatOpenAI(\n",
    "            api_key=OPENAI_API_KEY,\n",
    "            model=\"gpt-4o-mini\", # or \"gpt-4o-mini\" if cost/speed is a concern and quality is acceptable\n",
    "            temperature=0.5 # Slightly lower temp for more factual extraction\n",
    "        )\n",
    "        structured_llm_segment = llm_segment_extractor.with_structured_output(SegmentResponse)\n",
    "\n",
    "        for topic_index, current_topic in enumerate(identified_topics):\n",
    "            logging.info(f\"Processing topic {topic_index + 1}/{len(identified_topics)}: '{current_topic}'\")\n",
    "\n",
    "            segment_prompt_text = (\n",
    "                f\"From the provided full video transcript (which is a list of dictionaries, each with 'text', 'start' in seconds, and 'duration' in seconds), \"\n",
    "                f\"identify all distinct segments that are specifically and clearly about the topic: '{current_topic}'.\\n\\n\"\n",
    "                f\"For each segment you identify:\\n\"\n",
    "                f\"1. It MUST be between {segment_duration_min_seconds} and {segment_duration_max_seconds} seconds in duration.\\n\"\n",
    "                f\"2. Provide extremely accurate 'start_time' and 'end_time' in whole SECONDS from the beginning of the video.\\n\"\n",
    "                f\"3. Create a concise, engaging 'title' (max 10 words, relevant to the topic and segment content).\\n\"\n",
    "                f\"4. Write a brief 'description' (1-2 sentences) of what this specific segment covers.\\n\\n\"\n",
    "                f\"Carefully review the timestamps in the transcript to ensure accuracy. Prioritize segments that are coherent and self-contained.\\n\\n\"\n",
    "                f\"Respond ONLY with a JSON object containing a single key 'segments', where 'segments' is a list of these segment objects. \"\n",
    "                f\"If no segments match the criteria for this topic, return an empty list for 'segments'.\\n\\n\"\n",
    "                f\"Topic to focus on: {current_topic}\\n\\n\"\n",
    "                f\"Full Video Transcript (list of dictionaries):\\n\\\"\\\"\\\"\\n{raw_transcript_data}\\n\\\"\\\"\\\"\" # Send the structured transcript\n",
    "            )\n",
    "\n",
    "            segment_messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a YouTube shorts content producer. Your goal is to find precise video segments for a given topic based on a transcript. Respond ONLY with the requested JSON structure.\"},\n",
    "                {\"role\": \"user\", \"content\": segment_prompt_text}\n",
    "            ]\n",
    "\n",
    "            logging.info(f\"Invoking LLM for segment extraction on topic: '{current_topic}'...\")\n",
    "            try:\n",
    "                segment_extraction_result = structured_llm_segment.invoke(segment_messages)\n",
    "                if segment_extraction_result and segment_extraction_result.segments:\n",
    "                    logging.info(f\"Found {len(segment_extraction_result.segments)} segments for topic '{current_topic}'.\")\n",
    "                    for seg_obj in segment_extraction_result.segments:\n",
    "                        # Add calculated duration and topic to the segment data before storing\n",
    "                        calculated_duration = seg_obj.end_time - seg_obj.start_time\n",
    "                        # Convert Pydantic model to dict for easier appending if needed, or store as objects\n",
    "                        segment_dict = seg_obj.model_dump() # Pydantic V2\n",
    "                        segment_dict['calculated_duration'] = calculated_duration\n",
    "                        segment_dict['topic'] = current_topic # Keep track of which topic it came from\n",
    "                        all_extracted_segments.append(segment_dict)\n",
    "                else:\n",
    "                    logging.info(f\"No segments found by LLM for topic '{current_topic}'.\")\n",
    "            except Exception as e_invoke:\n",
    "                logging.error(f\"LLM invocation failed for topic '{current_topic}': {e_invoke}\", exc_info=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during the segment extraction process: {e}\", exc_info=True)\n",
    "else:\n",
    "    if not OPENAI_API_KEY:\n",
    "        logging.warning(\"OpenAI API key not available. Skipping segment extraction.\")\n",
    "    if not raw_transcript_data:\n",
    "        logging.warning(\"Transcript data not available. Skipping segment extraction.\")\n",
    "    if not identified_topics:\n",
    "        logging.warning(\"No topics identified. Skipping segment extraction.\")\n",
    "\n",
    "logging.info(f\"Total segments extracted across all topics: {len(all_extracted_segments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c8bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FFMPEG Snippet Generation ---\n",
    "if not all_extracted_segments:\n",
    "    logging.warning(\"No segments were extracted by the LLM. Skipping FFMPEG processing.\")\n",
    "elif not downloaded_video_path or not os.path.exists(downloaded_video_path):\n",
    "    logging.error(f\"Downloaded video path is not set or file does not exist: {downloaded_video_path}. Cannot generate clips.\")\n",
    "elif not video_base_title:\n",
    "    logging.error(\"Video base title is not set. Cannot generate clips.\")\n",
    "else:\n",
    "    logging.info(f\"Starting FFMPEG clip generation for {len(all_extracted_segments)} segments...\")\n",
    "    successful_clips = 0\n",
    "    failed_clips = 0\n",
    "\n",
    "    for i, segment_data in enumerate(all_extracted_segments):\n",
    "        try:\n",
    "            start_time = segment_data['start_time']\n",
    "            end_time = segment_data['end_time']\n",
    "            segment_title_raw = segment_data['title']\n",
    "            # description = segment_data['description'] # Available if needed\n",
    "            duration = segment_data.get('calculated_duration', end_time - start_time) # Use calculated or re-calculate\n",
    "\n",
    "            # Sanitize segment title for filename\n",
    "            segment_title_sanitized = \"\".join(c if c.isalnum() or c in (' ', '-', '_') else '_' for c in segment_title_raw).rstrip().replace(\" \", \"_\")\n",
    "            if not segment_title_sanitized: # Handle empty titles\n",
    "                segment_title_sanitized = f\"segment_{i}\"\n",
    "\n",
    "            output_clip_filename = f\"{video_base_title}_{segment_title_sanitized}_{duration}s_{i}.mp4\"\n",
    "            output_clip_path = os.path.join(clips_output_directory, output_clip_filename)\n",
    "\n",
    "            logging.info(f\"\\nProcessing segment {i + 1}/{len(all_extracted_segments)} for topic '{segment_data.get('topic','N/A')}':\")\n",
    "            logging.info(f\"  Source Video: {downloaded_video_path}\")\n",
    "            logging.info(f\"  Start: {start_time}s, End: {end_time}s, Duration: {duration}s\")\n",
    "            logging.info(f\"  Segment Title (Raw): {segment_title_raw}\")\n",
    "            logging.info(f\"  Output File: {output_clip_path}\")\n",
    "\n",
    "            # Construct FFMPEG command (ensure paths with spaces are quoted)\n",
    "            command = [\n",
    "                \"ffmpeg\",\n",
    "                \"-y\",  # Overwrite output files without asking\n",
    "                \"-hide_banner\",\n",
    "                \"-i\", downloaded_video_path,\n",
    "                \"-ss\", str(start_time),\n",
    "                \"-to\", str(end_time),\n",
    "                \"-c:v\", \"libx264\",\n",
    "                \"-crf\", \"18\",      # Constant Rate Factor (lower is better quality, 18 is visually lossless for many)\n",
    "                \"-preset\", \"medium\", # Encoding speed vs. compression (medium is a good balance)\n",
    "                \"-c:a\", \"aac\",\n",
    "                \"-b:a\", \"192k\",    # Audio bitrate\n",
    "                output_clip_path\n",
    "            ]\n",
    "            # Using list for command is safer with subprocess than f-string for complex commands\n",
    "\n",
    "            logging.info(f\"  Executing FFMPEG command: {' '.join(command)}\")\n",
    "            result = subprocess.run(command, capture_output=True, text=True, check=False)\n",
    "\n",
    "            if result.returncode == 0:\n",
    "                logging.info(f\"  Successfully generated clip: {output_clip_path}\")\n",
    "                successful_clips += 1\n",
    "            else:\n",
    "                logging.error(f\"  Error generating clip: {output_clip_path}\")\n",
    "                logging.error(f\"  FFMPEG STDOUT: {result.stdout.strip()}\")\n",
    "                logging.error(f\"  FFMPEG STDERR: {result.stderr.strip()}\")\n",
    "                failed_clips += 1\n",
    "        except KeyError as ke:\n",
    "            logging.error(f\"  Missing expected key in segment_data for segment {i}: {ke}. Segment data: {segment_data}\", exc_info=True)\n",
    "            failed_clips += 1\n",
    "        except FileNotFoundError:\n",
    "            logging.error(\"  Error: ffmpeg command not found. Please ensure ffmpeg is installed and in your system's PATH.\")\n",
    "            break # Stop processing further clips if ffmpeg is not found\n",
    "        except Exception as e:\n",
    "            logging.error(f\"  An unexpected error occurred generating clip for segment {i}: {e}\", exc_info=True)\n",
    "            failed_clips += 1\n",
    "    logging.info(f\"\\nFFMPEG processing finished. Successful clips: {successful_clips}, Failed clips: {failed_clips}\")\n",
    "    \n",
    "logging.info(\"--- YouTube Clipper Process Complete ---\")\n",
    "if successful_clips > 0 :\n",
    "    logging.info(f\"Review your generated clips in the '{clips_output_directory}' directory.\")\n",
    "else:\n",
    "    logging.info(\"No clips were successfully generated. Please review the logs for errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1d6995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_whisper\n",
    "# Or: import whisper\n",
    "import ffmpeg\n",
    "import json\n",
    "import os\n",
    "\n",
    "def extract_transcript_from_mp4(mp4_filepath, output_json_filepath=None, model_name=\"large\"):\n",
    "    \"\"\"\n",
    "    Extracts transcript with timestamps from an MP4 file using Whisper (via stable-ts).\n",
    "    ... (rest of the docstring) ...\n",
    "    \"\"\"\n",
    "    if not os.path.exists(mp4_filepath):\n",
    "        print(f\"DEBUG: Error - MP4 file not found at {mp4_filepath}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"DEBUG: Loading Whisper model '{model_name}'...\")\n",
    "    try:\n",
    "        model = stable_whisper.load_model(model_name)\n",
    "        # model = whisper.load_model(model_name) # If using original openai-whisper\n",
    "    except Exception as e:\n",
    "        print(f\"DEBUG: Error loading Whisper model: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"DEBUG: Transcribing '{mp4_filepath}'...\")\n",
    "    try:\n",
    "        # For stable-whisper, you might want to explore its specific transcribe options\n",
    "        # result = model.transcribe(mp4_filepath, fp16=False, word_timestamps=True, regroup=False) # Example for stable-whisper\n",
    "        result = model.transcribe(mp4_filepath, fp16=False) # Standard call\n",
    "\n",
    "        # --- DEBUGGING THE RESULT ---\n",
    "        print(\"\\n--- DEBUG: Full Whisper Transcription Result ---\")\n",
    "        if result is None:\n",
    "            print(\"DEBUG: result is None\")\n",
    "        else:\n",
    "            print(f\"DEBUG: type(result) = {type(result)}\")\n",
    "            if isinstance(result, dict):\n",
    "                print(f\"DEBUG: result.keys() = {result.keys()}\")\n",
    "                if 'text' in result:\n",
    "                    print(f\"DEBUG: result['text'] (first 500 chars) = {result['text'][:500]}\")\n",
    "                if 'segments' in result:\n",
    "                    print(f\"DEBUG: type(result['segments']) = {type(result['segments'])}\")\n",
    "                    if result['segments']:\n",
    "                        print(f\"DEBUG: len(result['segments']) = {len(result['segments'])}\")\n",
    "                        print(f\"DEBUG: First segment in result['segments'] = {result['segments'][0] if len(result['segments']) > 0 else 'N/A'}\")\n",
    "                    else:\n",
    "                        print(\"DEBUG: result['segments'] is empty.\")\n",
    "                else:\n",
    "                    print(\"DEBUG: 'segments' key NOT FOUND in result.\")\n",
    "            else:\n",
    "                # If using stable_whisper, the result might be a different object type\n",
    "                # For example, it might be a stable_whisper.result.WhisperResult object\n",
    "                # You'd need to inspect its attributes or methods as per stable_whisper docs\n",
    "                print(f\"DEBUG: result is not a dictionary. Raw result object: {result}\")\n",
    "                # Example for stable_whisper if result is an object:\n",
    "                # try:\n",
    "                #     print(f\"DEBUG: Accessing result.segments directly (for stable_whisper object)\")\n",
    "                #     segments_from_result_object = result.segments\n",
    "                #     if segments_from_result_object:\n",
    "                #         print(f\"DEBUG: len(segments_from_result_object) = {len(segments_from_result_object)}\")\n",
    "                #         print(f\"DEBUG: First segment from result object = {segments_from_result_object[0]}\")\n",
    "                #     else:\n",
    "                #         print(\"DEBUG: result.segments (object attribute) is empty or None\")\n",
    "                # except AttributeError:\n",
    "                #     print(\"DEBUG: result object does not have a .segments attribute directly accessible this way.\")\n",
    "\n",
    "\n",
    "        # --- Processing the result ---\n",
    "        transcript_segments = []\n",
    "        # Adjust access based on what the DEBUG prints show for 'result'\n",
    "        # The original openai-whisper returns a dict. stable-whisper might return an object\n",
    "        # that can be converted to a dict or accessed differently.\n",
    "\n",
    "        # Safely try to access segments\n",
    "        segments_data = None\n",
    "        if isinstance(result, dict) and 'segments' in result:\n",
    "            segments_data = result['segments']\n",
    "        elif hasattr(result, 'segments'): # For objects like stable_whisper.result.WhisperResult\n",
    "             # stable_whisper's .segments is often a generator or list of segment objects\n",
    "             # We might need to iterate and convert them to dicts\n",
    "             print(\"DEBUG: Accessing segments from result object attribute.\")\n",
    "             temp_segments = []\n",
    "             try:\n",
    "                for seg_obj in result.segments: # Iterate if it's a generator/list of objects\n",
    "                    # Convert stable-whisper segment object to a dictionary if needed\n",
    "                    # This depends on the exact structure of seg_obj from stable_whisper\n",
    "                    # A common pattern is:\n",
    "                    temp_segments.append({\n",
    "                        'text': getattr(seg_obj, 'text', '').strip(),\n",
    "                        'start': round(getattr(seg_obj, 'start', 0.0), 2),\n",
    "                        'end': round(getattr(seg_obj, 'end', 0.0), 2)\n",
    "                    })\n",
    "                segments_data = temp_segments\n",
    "                if segments_data:\n",
    "                    print(f\"DEBUG: Successfully extracted {len(segments_data)} segments from stable_whisper result object.\")\n",
    "                else:\n",
    "                    print(\"DEBUG: Failed to extract segments from stable_whisper result object or it was empty.\")\n",
    "\n",
    "             except Exception as e_seg_obj:\n",
    "                 print(f\"DEBUG: Error processing segments from stable_whisper object: {e_seg_obj}\")\n",
    "        \n",
    "        if segments_data: # Check if segments_data was successfully populated\n",
    "            for segment in segments_data:\n",
    "                # Ensure segment is a dict and has the required keys before proceeding\n",
    "                if isinstance(segment, dict) and 'text' in segment and 'start' in segment and 'end' in segment:\n",
    "                    transcript_segments.append({\n",
    "                        \"text\": segment['text'].strip(),\n",
    "                        \"start\": round(segment['start'], 2),\n",
    "                        \"end\": round(segment['end'], 2),\n",
    "                        \"duration\": round(segment['end'] - segment['start'], 2)\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"DEBUG: Skipping malformed segment: {segment}\")\n",
    "\n",
    "            if transcript_segments:\n",
    "                print(f\"DEBUG: Transcription processing successful. Found {len(transcript_segments)} formatted segments.\")\n",
    "            else:\n",
    "                print(\"DEBUG: No valid segments found after processing the transcription result.\")\n",
    "                # This could happen if segments_data was populated but items were malformed\n",
    "        else:\n",
    "            print(\"DEBUG: 'segments_data' is None or empty. Transcription did not produce expected segments.\")\n",
    "            return None # Exit if no segments were found in the result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"DEBUG: Error during transcription processing: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Save to JSON\n",
    "    if not transcript_segments: # Double check before saving\n",
    "        print(\"DEBUG: No transcript segments to save.\")\n",
    "        return None\n",
    "\n",
    "    if output_json_filepath is None:\n",
    "        base, ext = os.path.splitext(mp4_filepath)\n",
    "        output_json_filepath = f\"{base}_transcript_whisper.json\"\n",
    "\n",
    "    try:\n",
    "        with open(output_json_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(transcript_segments, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"DEBUG: Transcript saved to: {output_json_filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"DEBUG: Error saving transcript to JSON: {e}\")\n",
    "\n",
    "    return transcript_segments\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    video_download_directory_for_test = \"downloaded_video\"\n",
    "    test_video_filename = \"The_High-Paying_AI_Job_Nobody_Knows_About__Yet__ft__Rachel_Woods.mp4\"\n",
    "    mp4_file_to_test = os.path.join(video_download_directory_for_test, test_video_filename)\n",
    "\n",
    "    print(f\"Attempting to test transcription with: {mp4_file_to_test}\")\n",
    "\n",
    "    if not os.path.exists(mp4_file_to_test):\n",
    "        print(f\"Error: The test MP4 file '{mp4_file_to_test}' does not exist. Please check the path.\")\n",
    "    else:\n",
    "        transcript_data = extract_transcript_from_mp4(mp4_file_to_test, model_name=\"large\")\n",
    "\n",
    "        if transcript_data:\n",
    "            print(\"\\n--- First 5 Transcript Segments (from example usage) ---\")\n",
    "            for i, segment_info in enumerate(transcript_data[:5]):\n",
    "                print(f\"Segment {i+1}:\")\n",
    "                print(f\"  Start: {segment_info['start']:.2f}s, End: {segment_info['end']:.2f}s, Duration: {segment_info['duration']:.2f}s\")\n",
    "                print(f\"  Text: {segment_info['text']}\")\n",
    "        else:\n",
    "            print(\"\\nExample usage: Transcription failed or produced no data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1078de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version PyTorch was compiled with: {torch.version.cuda}\") # This should ideally be 12.1\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA is NOT available to PyTorch. Check installation and compatibility.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5117e89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc # Garbage Collector interface\n",
    "\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    del model # Remove your reference to the model object\n",
    "    print(\"Deleted model variable.\")\n",
    "\n",
    "# Explicitly clear PyTorch's CUDA memory cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Cleared PyTorch CUDA cache.\")\n",
    "    # You can also run garbage collection for good measure,\n",
    "    # though empty_cache is usually the more direct for GPU.\n",
    "    gc.collect()\n",
    "    print(\"Ran Python garbage collection.\")\n",
    "\n",
    "# After this, check nvidia-smi. The memory used by that model should be freed.\n",
    "# Note: Some base CUDA context memory (~few hundred MBs) might remain as long as PyTorch is active."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
